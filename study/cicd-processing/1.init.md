# CICD 및 인프라 세팅

## 1. 최초세팅

이번에 해당 프로젝트에서 CICD와 인프라를 담당하게 됐다. 이전에는 단순히 React 배포정도만 경험해봐서 S3 + Cloundfront 정도의 CICD는 익숙한 편이지만 실무차원에서 사용되는 ECS+ECR 조합은 직접 세팅해본적이 없어 이번 기회를 통해 직접 구성해볼 계획이다.

우선은 해당 프로젝트는 `React`로 만든 admin인 `digging-admin-front` 와 실제 유저가 사용하는 `Next.js`로 만든 `digging-front` 그리고 서버는 `Nest.js` 로 만들었다. 나중에 admin 서버가 추가 될 수는 있으나 우선은 이정도로 프로젝트 세팅을 시작해 보도록 하겠다.

## 2. 기본컨셉

### 🐳 컨테이너 배포 컨셉

기본 컨셉은 위에 작성했듯이 ECR+ECS를 사용할 예정이다. 처음에는 ECR말고 Docker Hub를 사용할까 했으나 아직 본인이 데브옵스 개발자도 아니고 우선 실무적 차원에서 자주 사용하는 (즉 본인의 현재 회사에서는 해당 시스템을 사용중임) 조합으로 학습해보는게 낫다는 판단하에 모두 AWS를 사용하기로 결정했다.

### 🔧 CI/CD 전략

CI는 일단 Github Actions를 사용할 예정이다. 원래는 Jenkins를 사용해볼까 했는데 요즘 굳이 서버가 필요한 Jenkins를 사용해야하나 싶은 마음도 있고 거스를 수 없는 트렌드가 Jenkins 사용을 지양하는 쪽으로 가고 있기 때문이다.

그런데 CD는 아직 고민중이다 Github Actions로 전부 통합할지 아니면 AWS Code Deploy를 사용할지 해당 부분은 우선 CI를 구축해놓고 생각해보기로 하겠다.

## 3. 진행과정

### 0. 한 폴더에 여러 프로젝트를 다 넣은 이유

우선 우선 로컬에서 각 프로젝트를 도커로 실행시키는 작업먼저 진행하기전에 왜 한 폴더 즉 하나에 레포에 모든 프로젝트를 넣었는지 알아보도록 하자

우선 본인은 하나의 CICD는 하나의 레포에서만 가능한줄 알았다. 그래서 최초에는 각 폴더별로 깃허브 레포를 만들어서 관리해서 CICD를 구축할 예정이었다. 그런데 그렇게되면 해당 폴더에 있는 .env 파일이랑 docker-compose 파일을 어떻게 관리해야하는 의문이 들었는데 찾아보니까 하나의 레포에서 특정 폴더에서만 push가 됐을 때 즉 특정 폴더에서 코드가 변경됐으면 해당 폴더를 바라보는 파이프라인을 만들어서 각각의 폴더별로 CICD를 구축 할 수 있다는 사실을 처음알게 됐다 생각해보면 모노레포에서도 비슷한 방법으로 각 프로젝트별로 CICD를 구축할 수 있으니 해당 방법도 가능한게 아닌가 싶다. 그래서 해당 방법에 대해서는 위대하신 GPT에게 질문한 답변을 적어두고 다음 스텝으로 이동하도록 하겠다.

```md
## 💡 폴더 기반 CI/CD 전략 요약

- 하나의 레포 내에서도 특정 폴더에 변경이 있을 때만 CI/CD를 실행하는 구조가 가능하다
- GitHub Actions의 `paths` 또는 `if` 조건을 활용하여 디렉토리 기반 트리거 설정
- 모노레포 환경에서도 동일하게 각 앱/패키지 단위로 독립된 워크플로우를 구성할 수 있다
- 환경 파일(`.env`)과 `docker-compose.yml`은 폴더 내에 두고, GitHub Actions에서 각 경로에 맞게 참조하거나 secrets로 주입 가능하다
```

### 1. Next DockerFile 작성

사실 도커파일 작성하는건 너무 쉬워서 생략할까 했으나 그래도 계속해서 개선해나가는 과정을 작성하기 위해 작성하도록하겠다. 그리고 복습하는 의미에서 각 프로젝트의 DocerFile 명령어를 확인까지 해보도록 하겠다.

**digging-front(Next.js)**

```Dockerfile
FROM node:20.11.0-alpine AS builder
WORKDIR /app
COPY package.json package-lock.json ./
RUN npm ci
COPY . .
RUN npm run build
FROM node:20.11.0-alpine
WORKDIR /app
COPY --from=builder /app/package.json ./
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/.next ./.next
COPY --from=builder /app/public ./public
EXPOSE 3001
CMD ["npx", "next", "start"]
```

우선 FROM 부터 확인하면 해당 명령어는 해당 이미지를 기반으로 새 이미지를 만들겠다는 의미이다. 즉 Node.js 공식 Docker 이미지를 이용한다는 의미이고 alpinedms 경량 리눅스기반이라 대충 뭐 빠르고 좋다는 의미이다. AS builder는 해당 이미지에 별칭을 붙이는것이고 나중에 재사용이 가능하다. 이는 빌더 단계에서만 node_modules 설치하고 실제 이미지에는 빌드된 결가만 가져오는 구조라고 보면된다.

이제 이런 방법론을 **multi-stage build** 라고 하는데 그러면 도대체 이게 무엇일가 ? 우선 전지전능하신 GPT님에게 질문을 해보자

> 여러 개의 FROM을 사용해서 각 단계별로 목적에 따라 다른 작업을 수행하고,
> 최종 이미지는 가장 필요한 것만 담아서 가볍게 만드는 기술이야.

음... 대충 무슨 의미인지는 알겠으나 조금 더 자세히 알아보기 위해 본인의 코드를 통해서 살펴보도록 하자

```dockerfile
FROM node:20.11.0-alpine AS builder
WORKDIR /app
COPY package.json package-lock.json ./
RUN npm ci
COPY . .
RUN npm run build
```

해당 단계에서는 우선 스테이지를 build로 정의해준다 그리고 npm ci를 통해서 정확한 의존성을 설치해주고 npm run build를 통해서 .next 폴더 를 생성해준다 아 참고로 .next 폴더는 최종 빌드된 결과물이다

그리고 다음 스테이지에서도 동일하게 node를 사용해준다 그리고 그냥 빌드된 결과물과 배포하는데 필요한 것 들만 뽑아와주는거다. 여기서 필요없는 package-lock.json 이나 src/ tsconfig.json 같은건 안넣어주는거다

그런데 여기서 또 한가지 의문점이 생긴다 stage를 여러개 만든다는 개념은 이해했는데 그렇다면 이전에 만든 stage는 어떻게 되는것이지? 라는 의문점이 생길 것이다.

결론부터 말하면 multi-stage 빌드는 가장 마지막에 있는 FROM 블록만 최종이미지로 사용한다. 대신 최종 이미지를 만들기전에는 다른 스테이지에 있는 필요한 결과들을가져올 수 있고 최종이미지가 만들어지면 이전 스테이지는 자동적으로 폐기된다.

자 물론 지금은 우선 컨테이너 생성하는것을 목적으로하기 위해서 최적화 하지 않았다. 나중에는 캐싱 및 도커이그노어 설정 standalone 설정을 통해서 최적화를 할 수 있는데 우선은 여기까지만 살펴보도록 하자

### 2. React Dockerfile 작성

자 그러면 이제 React Dockerfile 작성도 살펴보도록하자

```dockerfile
FROM node:20.11.0-alpine AS builder
WORKDIR /app
COPY package.json package-lock.json ./
RUN npm ci
COPY . .
RUN npm run build
FROM nginx:alpine
COPY --from=builder /app/dist /usr/share/nginx/html
EXPOSE 80
CMD ["nginx", "-g", "daemon off;"]
```

자 이거도 어렵지 않다 그냥 node 환경에서 의존성 설치해주고 빌드해주고 빌드된 파일을 nginx 스테이지 새로만들어서 복붙해지고 ngninx 실행해주는 로직이다.

물론 리액트도 gzip, cache-control 등등 고급설정을 해줘서 최적화를 시켜줄 수 있지만 그거도 우선은 나중에 생각해보자

### 3. Nest Dockerfile 작성

```dockerfile
FROM node:20.11.0-alpine AS builder
WORKDIR /app
COPY package.json yarn.lock ./
RUN yarn install --frozen-lockfile
COPY . .
RUN yarn build
FROM node:20.11.0-alpine
WORKDIR /app
COPY --from=builder /app/package.json ./
COPY --from=builder /app/node_modules ./node_modules
COPY --from=builder /app/dist ./dist
EXPOSE 3002
CMD ["node", "dist/main"]
```

자 이제 우리는 쫄 필요가없다. 이거도 다 똑같은데 다른점이라곤 dist 폴더에 있는걸 직접 실행시켜줄 뿐이다.

자 그러면 빨리 마지막 단계로 넘어가보자

### 4. docker-compose.yml

```yaml
# Docker Compose 파일 버전 (최신 기능을 위해 3.8 사용)
version: "3.8"

services:
  # digging-admin-front (React 앱 서비스, 정적 파일을 Nginx로 서빙)
  digging-admin-front:
    build:
      context: ./digging-admin-front
      dockerfile: Dockerfile
    # 호스트의 3001번 포트를 컨테이너의 80번 포트(Nginx)로 연결
    ports:
      - "3001:80"
    networks:
      - app-network

  # digging-front (Next.js 앱 서비스, SSR 또는 CSR 기반 프론트엔드)
  digging-front:
    build:
      context: ./digging-front
      dockerfile: Dockerfile
    # 호스트의 3000번 포트를 컨테이너의 3000번 포트로 연결 (Next.js 기본 포트)
    ports:
      - "3000:3000"
    networks:
      - app-network

  # digging-back (Nest.js 백엔드 API 서비스)
  digging-back:
    build:
      context: ./digging-back
      dockerfile: Dockerfile
    depends_on:
      - digging-db
    environment:
      # DB 접속 정보 역시 .env에서 가져옴
      - DB_HOST=${DB_HOST}
      - DB_PORT=${DB_PORT}
      - DB_NAME=${POSTGRES_DB}
      - DB_USER=${POSTGRES_USER}
      - DB_PASS=${POSTGRES_PASSWORD}
    # 호스트와 컨테이너 모두 3002번 포트를 사용 (Nest.js 기본 포트)
    ports:
      - "3002:3002"
    networks:
      - app-network

        # PostgreSQL 데이터베이스 서비스
  # PostgreSQL 데이터베이스 서비스
  digging-db:
    image: postgres:13 # PostgreSQL 공식 이미지
    restart: always # 중단 시 자동 재시작
    environment:
      # .env에 정의한 변수 참조
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
    volumes:
      - db_data:/var/lib/postgresql/data
    ports:
      # 호스트 포트도 .env로 관리
      - "${DB_PORT}:5432"
    networks:
      - app-network

# 영구 데이터 저장을 위한 볼륨
volumes:
  db_data:

networks:
  app-network:
    driver: bridge
```

자 물론 이거도 처음으로 접하는 사람은 두가 깨질 수 있다 하지만 본인은 이미 여러번 해당 작업을 해봤으므로 몇가지 명령어만 집고 넘어가도록 하겠다.

```yaml
build:
  context: ./digging-admin-front
  dockerfile: Dockerfile
```

context는 빌드 작업의 루트 디렉토리이다 그러니까 해당 경로를 기준으로 작업을 수행하겠다는 의미이가
dockerfile은 사용할 Dockerfile 경로인데 만약에 파일명이 Dockerfile이 아니라면 명시적으로 작성해줘야한다.

즉 정리하면 즉, `./digging-admin-front/Dockerfile`을 기준으로 `/digging-admin-front` 내 파일들로 이미지 빌드한다는 의미이다

```yaml
depends_on:
  - digging-db
```

해당 서비스는 digging-db가 먼저 시작되어야 컨테이너가 실행된다는 것을 의미한다.

무론 지금은 DB가 완전히 준비됨을 보장하지 않는다 이는 나중에 헬스체크를 추가해야하는데 이것도 나중에 천천히 확인해보도록 하자

```yaml
networks:
  app-network:
    driver: bridge
```

bridge는 Docker 컨테이너 간 통신을 위한 기본 가상 네트워크를 의미한다 여기서는 모든 서비스가 app-network에 붙어 있어서 서로 내부 호스트네임으로 접근 가능하다는 의미인데 사실 이부분을 자세히 몰라서 나중에 다시 확인해보도록 하자

그리고 또 추가적으로 .env.template와 --env-file 경로설정 방법도 천천히 알아보도록 하자

## 다음에 확인해 볼 것들

### 1. CI/CD 전략

- [ ] CD 방식 확정 (GitHub Actions 단일 vs AWS CodeDeploy 분리)
- [ ] 환경별 배포 전략 문서화 (dev/stage/prod 구분)

### 2. Next.js (digging-front) Docker 최적화

- [ ] `.dockerignore` 설정 최적화
- [ ] `output: 'standalone'` 적용 (`next.config.js`)
- [ ] `npm ci --omit=dev` 혹은 빌드 후 `prune`로 devDependencies 제거
- [ ] Docker layer 캐싱 최적화 (`COPY package*.json → npm ci → COPY . .`)

### 3. React (digging-admin-front) Nginx 최적화

- [ ] gzip 설정 추가
- [ ] cache-control, expires 등 헤더 최적화
- [ ] fallback 라우팅 설정 (`try_files $uri /index.html`)

### 4. 헬스체크 및 의존성 준비

- [ ] digging-db에 `healthcheck` 설정 (`pg_isready`, interval/retries 등)
- [ ] digging-back에 `wait-for-it.sh` 또는 equivalent DB-ready script 적용

### 5. Docker 네트워크 구조 이해

- [ ] bridge 네트워크 구조 개념 정리
- [ ] `docker inspect network` 명령어로 서비스 확인 실습
- [ ] 서비스 간 접근을 호스트네임(digging-db 등) 기반으로 확인
